{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">CUDA C/C++ 통합 메모리로 가속 애플리케이션 메모리 관리</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*CUDA 모범 사례 가이드*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)는 이번 실습과 다른 CUDA 기본 실습 이후에 후속 학습하면 좋은 콘텐츠로, **A**ssess(평가), **P**arallelize(병렬화), **O**ptimize(최적화), **D**eploy(배포)의 약자인 **APOD** 디자인 주기를 권장합니다. 즉, APOD는 개발자가 가속 애플리케이션의 성능에 증분 개선을 적용하고 코드를 배포할 수 있는 반복적 디자인 프로세스를 처방하는 것입니다. 개발자가 더 유능한 CUDA 프로그래머가 되기 시작하면 가속 코드베이스에 더 고급 수준의 최적화 기법을 적용할 수 있습니다.\n",
    "\n",
    "이 실습은 이러한 방식의 반복 개발을 지원할 것입니다. Nsight Systems 명령줄 도구인 **nsys**를 사용하여 애플리케이션의 성능을 정성적으로 측정하고, 최적화 기회를 파악하기 위해 새로운 기술을 학습하고 주기를 반복하기 전에 증분 개선을 적용합니다. 요점은 이 실습에서 학습하고 적용하는 많은 기법이 CUDA의 **통합 메모리**가 작동하는 방법의 세부 사항을 다루게 된다는 것입니다. 통합 메모리 동작을 이해하는 것은 CUDA 개발자에게 매우 기본적인 역량으로, 더 많은 고급 메모리 관리 기법에 있어 전제적인 역할을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 전제 조건\n",
    "\n",
    "이 실습을 최대한 잘 활용하려면 여러분은 이미 다음을 하실 수 있어야 합니다.\n",
    "\n",
    "- CPU 함수를 호출하고 GPU 커널을 실행하는 C/C++ 프로그램 작성, 컴파일, 실행\n",
    "- 실행 구성을 이용한 병렬 스레드 계층 구조 제어\n",
    "- 직렬 루프를 GPU에서 병렬로 반복 실행하도록 리팩터링\n",
    "- 통합 메모리 할당 및 해제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 목표\n",
    "\n",
    "이 실습을 완료할 때는 다음을 하실 수 있게 됩니다.\n",
    "\n",
    "- Nsight Systems 명령줄 도구(**nsys**)를 사용하여 가속 애플리케이션 성능 프로파일링\n",
    "- **스트리밍 멀티프로세서**에 대한 이해를 활용하여 실행 구성 최적화\n",
    "- 페이지 폴트 및 데이터 마이그레이션에 관한 **통합 메모리** 동작 이해\n",
    "- 성능 향상을 위해 **비동기 메모리 프리페치**를 사용하여 페이지 폴트 및 데이터 마이그레이션 감소\n",
    "- 애플리케이션을 빠르게 가속화 및 배포하기 위해 반복 개발 주기 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NVIDIA 명령줄 프로파일러를 통한 반복 최적화\n",
    "\n",
    "가속 코드베이스 최적화 시도가 실제로 성공적이었다는 것을 확인할 수 있는 유일한 방법은 애플리케이션의 성능에 대한 정량적 정보를 위해 애플리케이션을 프로파일링하는 것입니다. `nsys`는 Nsight Systems 명령줄 도구입니다. CUDA 툴킷과 함께 제공되며, 가속 애플리케이션 프로파일링을 위한 강력한 도구입니다.\n",
    "\n",
    "`nsys`는 사용하기 쉽습니다. 가장 기본적인 사용법은 단순히 `nvcc`로 컴파일된 실행 파일을 경로로 전달하는 것입니다. `nsys`는 애플리케이션이 애플리케이션의 GPU 활동, CUDA API 호출 요약뿐 아니라 이 실습의 후반부에 광범위하게 다루게 될 주제인 **통합 메모리** 활동에 대한 정보를 출력하고 난 뒤 계속해서 애플리케이션을 실행합니다.\n",
    "\n",
    "애플리케이션을 가속화하거나 이미 가속화된 애플리케이션을 최적화할 때는 과학적이고 반복적인 접근 방식을 취하세요. 변경 사항을 적용한 후 애플리케이션을 프로파일링하고, 메모하고, 성능에 대한 리팩터링의 영향을 기록하세요. 이러한 관찰을 일찍, 자주 하세요. 가속 애플리케이션을 보내는 것 정도의 약간의 노력으로 충분한 성능 향상을 얻을 수 있는 경우가 빈번히 있습니다. 또한, 프로파일링을 자주 하면 CUDA 코드베이스에 대한 특정 변경 사항이 실제 성능에 어떤 영향을 미치는지 배울 수 있습니다. 코드베이스에서 여러 유형의 변경을 거친 후에만 프로파일링을 하는 경우에는 확보하기 어려운 지식이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: nsys로 애플리케이션 프로파일링\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ 이 링크를 포함하여 이 실습의 모든 소스 파일 링크를 클릭하면 열어서 편집할 수 있습니다)는 단순하게 가속화된 벡터 추가 프로그램입니다. 아래 2개의 코드 실행 셀을 사용하세요(`CTRL` + 클릭). 첫 번째 코드 실행 셀은 벡터 추가 프로그램을 컴파일 (및 실행)합니다. 두 번째 코드 실행 셀은 방금 `nsys profile`을 사용하여 컴파일한 실행 파일을 프로파일링합니다.\n",
    "\n",
    "`nsys profile`은 다양한 방식으로 활용할 수 있는 `qdrep` 보고서 파일을 생성합니다. `--stats=true` 플래그를 사용하여 출력된 요약 통계를 표시하겠습니다. 출력된 정보가 꽤 많습니다.\n",
    "\n",
    "- 프로파일 구성 세부 정보\n",
    "- 보고서 파일 세대 세부 정보\n",
    "- **CUDA API 통계**\n",
    "- **CUDA 커널 통계**\n",
    "- **CUDA 메모리 작업 통계(시간 및 크기)**\n",
    "- OS 런타임 API 통계\n",
    "\n",
    "이 실습에서는 **굵게** 처리된 3개 섹션을 주로 사용하게 됩니다. 다음 실습에서는 생성된 보고서 파일을 사용하여 Nsight Systems에 비주얼 프로파일링을 위한 GUI를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "애플리케이션을 프로파일링한 후 프로파일링 출력에 표시되는 정보를 사용하여 다음 질문에 답하세요.\n",
    "\n",
    "- 이 애플리케이션에서 호출된 유일한 CUDA 커널의 이름은 무엇인가요?\n",
    "- 이 커널은 몇 번 실행되었나요?\n",
    "- 이 커널을 실행하는 데 얼마나 걸렸나요? 이 시간을 어딘가에 기록해 두세요. 이 애플리케이션을 최적화할 때 얼마나 더 빠르게 만들 수 있는지 궁금하실 테니까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언급하고 넘어갈 만한 것은 기본적으로 `nsys profile`가 기존 보고서 파일을 덮어쓰지 않는다는 점입니다. 이는 프로파일링 시 작업이 실수로 손실되는 것을 방지하기 위해 수행됩니다. 어떤 이유로든, 가령 빠른 반복 작업 중에 기존 보고서 파일을 덮어쓰기를 원한다면, `-f` 플래그를 `nsys profile`에 제공해 기존 보고서 파일 덮어쓰기를 허용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 최적화 및 프로파일링\n",
    "\n",
    "1-2분 정도의 시간을 내어 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu)가 단일 스레드 블록의 많은 스레드에서 실행되도록 실행 구성을 업데이트하여 간단한 최적화를 진행하세요. 다시 컴파일한 다음 아래의 코드 실행 셀을 사용하여 `nsys profile --stats=true`로 프로파일링하세요. 프로파일링 출력을 사용하여 커널의 런타임을 확인하세요. 이 최적화로 속도가 얼마나 향상되었나요? 결과를 어딘가에 기록해 두세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 반복 최적화\n",
    "\n",
    "이 연습문제에서는 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu)의 실행 구성을 편집하고 프로파일링한 뒤 영향을 확인하기 위해 결과를 기록하는 여러 주기를 거치게 됩니다. 작업하는 동안 다음 가이드라인을 활용하세요.\n",
    "\n",
    "- 실행 구성을 업데이트하는 3~5가지의 다양한 방법을 나열하는 것으로 시작하여 다양한 그리드와 블록 크기 조합을 다루도록 하세요.\n",
    "- [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 프로그램을 여러분이 나열한 방법 중 하나로 편집하세요.\n",
    "- 아래 2개의 코드 실행 셀을 사용하여 업데이트된 코드를 컴파일 및 프로파일링하세요.\n",
    "- 프로파일링 출력에 나타나는 대로 커널 실행의 런타임을 기록하세요.\n",
    "- 위에서 가능하다고 나열한 각각의 최적화 방법에 대해 편집/프로파일/기록 주기를 반복하세요.\n",
    "\n",
    "시도한 실행 구성 중 가장 빠른 것으로 증명된 구성은 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 멀티프로세서 스트리밍 및 디바이스 쿼리\n",
    "\n",
    "이 섹션에서는 GPU 하드웨어의 특정 특징에 대한 이해를 통해 최적화의 수준을 올릴 수 있는 방법을 알아봅니다. **스트리밍 멀티프로세서** 개론을 배운 후, 여러분이 작업 중인 가속 벡터 추가 프로그램을 더욱 최적화해 보게 됩니다.\n",
    "\n",
    "다음 슬라이드에서는 앞으로 다룰 콘텐츠를 고수준에서 시각적으로 보여드립니다. 다음 섹션으로 넘어가 각 주제를 더 자세히 다루기 전에 슬라이드를 클릭해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRByDOlhmGKNY9IgFonAhE-uM0NAPdZGo8v8vlBBPqRB7RDx-E5g0OnGOpC2VoO-eWFhZBWv5yCtGfk/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍 멀티프로세서 및 워프\n",
    "\n",
    "CUDA 애플리케이션이 실행되는 GPU에는 **스트리밍 멀티프로세서** 또는 **SM**이라고 불리는 처리 단위가 있습니다. 커널이 실행되는 동안 스레드 블록은 실행을 위해 SM에 주어집니다. 가능한 한 많은 병렬 연산을 수행하도록 GPU의 성능을 지원하기 위해, *주어진 GPU에서 SM 수의 배수인 블록 개수를 가지는 그리드 크기를 선택하는 것*으로 성능 향상을 얻을 수 있습니다.\n",
    "\n",
    "또한 SM은 **워프**라고 불리는 블록 내에서 32개 스레드의 그룹을 생성, 관리, 스케쥴링 및 실행합니다. [SM 및 워프의 심화 내용](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)을 더 다루는 것은 이 과정의 범위 밖이지만, *스레드 수가 32의 배수인 블록 크기를 선택*함으로써 성능 향상을 얻을 수 있다는 것을 아는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로그래밍적으로 GPU 디바이스 속성 쿼리하기\n",
    "\n",
    "GPU의 SM 수는 사용되는 특정 GPU에 따라 다를 수 있으므로, 이동성을 지원하기 위해서는 코드베이스에 SM의 수를 하드 코딩하지 말아야 합니다. 이러한 정보는 프로그래밍적으로 취득해야 합니다.\n",
    "\n",
    "다음은 CUDA C/C++에서 SM의 수를 포함하여 현재 활성 GPU 디바이스에 대한 여러 속성이 포함된 C 구조체를 얻는 방법을 보여줍니다.\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 디바이스 쿼리\n",
    "\n",
    "현재 [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu)에는 수많은 미지정 변수가 포함되어 있으며, 현재 활성 GPU에 대한 세부 정보를 설명하도록 의도된 이런저런 정보를 출력합니다.\n",
    "\n",
    "[`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu)가 소스 코드에 나타난 원하는 디바이스 속성의 실제 값을 출력하도록 빌드를 만드세요. 여러분의 작업을 지원하고 문서를 소개하는 의미로, [CUDA 런타임 문서](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html)를 사용하여 디바이스 속성 구조체의 관련 속성을 파악하는 데 도움을 받으세요. 도중에 막히면 [해답](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu)을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: SM의 수에 맞는 크기의 그리드를 통해 벡터 추가 최적화\n",
    "\n",
    "디바이스에 SM의 수를 쿼리하는 능력을 활용해 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 내에서 작업 중인 `addVectorsInto` 커널을 리팩터링하여 디바이스 SM 수의 배수와 같은 개수의 블록을 포함하는 그리드에서 실행되도록 하세요.\n",
    "\n",
    "작성한 코드의 다른 특정 세부 정보에 따라 이 리팩터는 커널의 성능에 개선 또는 현저한 변화를 줄 수도, 주지 않을 수도 있습니다. 따라서, 언제나 그렇듯이, `nsys profile`을 사용하여 성능 변경을 정량적으로 평가할 수 있도록 하세요. 프로파일링 출력 콘텐츠를 기반으로 지금까지의 결과를 나머지 발견들과 함께 기록하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-vector-add/01-vector-add.cu(70): warning: variable \"numberOfBlocks\" was set but never used\n",
      "\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-4bc7-ef3b-0146-6256.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-4bc7-ef3b-0146-6256.qdrep\"\n",
      "Exporting 11937 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-4bc7-ef3b-0146-6256.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    59.0        223093784          3   74364594.7      18992  223018978  cudaMallocManaged    \n",
      "    35.3        133678604          1  133678604.0  133678604  133678604  cudaDeviceSynchronize\n",
      "     5.7         21414477          3    7138159.0    6525463    8248210  cudaFree             \n",
      "     0.0            50745          1      50745.0      50745      50745  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        133658262          1  133658262.0  133658262  133658262  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    79.6         82302627       10083   8162.5     1823   162240  [CUDA Unified Memory memcpy HtoD]\n",
      "    20.4         21137888         768  27523.3     1631   160064  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000       10083   38.998    4.000   972.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    53.9       1333336208         72  18518558.4    17288  100128197  poll          \n",
      "    41.4       1024002253         72  14222253.5     9488  100071305  sem_timedwait \n",
      "     3.6         89655217        602    148928.9     1065   16119661  ioctl         \n",
      "     0.9         23463695         90    260707.7     1130    8164542  mmap          \n",
      "     0.0           664329         77      8627.6     2323      21129  open64        \n",
      "     0.0           136438          4     34109.5    31839      36039  pthread_create\n",
      "     0.0           121092         23      5264.9     1396      20033  fopen         \n",
      "     0.0            96178         11      8743.5     4269      15904  write         \n",
      "     0.0            81430          3     27143.3    20679      37819  fgets         \n",
      "     0.0            51766         14      3697.6     1107       6424  munmap        \n",
      "     0.0            34895          5      6979.0     2705      10722  open          \n",
      "     0.0            29740         16      1858.8     1036       3613  fclose        \n",
      "     0.0            27711         11      2519.2     1053       3803  read          \n",
      "     0.0            15593          2      7796.5     7179       8414  socket        \n",
      "     0.0            14280          3      4760.0     4268       5132  pipe2         \n",
      "     0.0             9163          2      4581.5     4393       4770  fread         \n",
      "     0.0             8127          2      4063.5     1501       6626  fgetc         \n",
      "     0.0             7748          1      7748.0     7748       7748  connect       \n",
      "     0.0             7543          3      2514.3     1076       5316  fcntl         \n",
      "     0.0             6630          4      1657.5     1544       1728  mprotect      \n",
      "     0.0             2505          1      2505.0     2505       2505  bind          \n",
      "     0.0             1421          1      1421.0     1421       1421  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 통합 메모리 세부 정보\n",
    "\n",
    "`cudaMallocManaged`를 사용하여 호스트 또는 디바이스 코드에 사용할 메모리를 할당했는데, 지금까지는 **통합 메모리**(**UM**)가 실제 작업에서 `cudaMallocManaged`에 의해 어떻게 할당되는지에 대한 세부 콘텐츠를 알아보지 않고 자동 메모리 마이그레이션, 프로그래밍 용이성 등 이 방법의 이점을 누렸습니다.\n",
    "\n",
    "`nsys profile`은 가속 애플리케이션의 UM 관리에 대한 세부 정보를 제공하며, 이 정보를 UM의 작동 방식에 대한 자세한 이해와 함께 사용하여 가속 애플리케이션을 최적화할 추가적인 기회를 제공합니다.\n",
    "\n",
    "다음 슬라이드에서는 앞으로 다룰 콘텐츠를 고수준에서 시각적으로 보여드립니다. 다음 섹션으로 넘어가 각 주제를 더 자세히 다루기 전에 슬라이드를 클릭해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS0-BCGiWUb82r1RH-4cSRmZjN2vjebqoodlHIN1fvtt1iDh8X8W9WOSlLVxcsY747WVIebw13cDYBO/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통합 메모리 마이그레이션\n",
    "\n",
    "UM이 할당되면, 메모리는 호스트 또는 디바이스에 아직 상주하지 않는 상태입니다. 호스트 또는 디바이스가 메모리에 액세스하려고 시도하면 호스트 또는 디바이스가 필요한 데이터를 배치로 마이그레이션하는 시점에 [페이지 폴트](https://en.wikipedia.org/wiki/Page_fault)가 일어납니다. 이와 유사하게, CPU 또는 가속 시스템의 GPU가 아직 상주하지 않는 메모리에 액세스하려고 시도할 때 페이지 폴트가 발생하고 마이그레이션이 트리거됩니다.\n",
    "\n",
    "온디맨드 페이지 폴트 및 메모리 마이그레이션 능력은 가속 애플리케이션의 개발 용이성에 엄청난 도움이 됩니다. 또한, 희소한 액세스 패턴을 보이는 데이터로 작업하는 경우(예: 애플리케이션이 실제로 실행될 때까지 어떤 데이터를 처리해야 하는지 알 수 없는 경우) 및 여러 개의 GPU가 탑재된 가속 시스템의 여러 GPU 디바이스에서 데이터에 액세스할 수 있는 시나리오의 경우, 온디맨드 메모리 마이그레이션이 상당히 유용합니다.\n",
    "\n",
    "예를 들어 런타임 이전에 데이터 요구 사항이 알려져 있고 대규모 연속 메모리 블록이 필요한 경우 페이지 폴트 및 온디맨드 데이터 마이그레이션 오버헤드가 되도록 피하는 것이 좋은 오버헤드 비용을 발생시킬 때가 있습니다.\n",
    "\n",
    "이 실습의 나머지 콘텐츠 대부분은 온디맨드 마이그레이션을 이해하고 프로파일러의 출력값에서 이를 파악하는 방법을 이해하는 데 할애됩니다. 이 지식을 시나리오에서 유용하게 활용하여 오버헤드를 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: UM 마이그레이션 및 페이지 폴트 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile`은 프로파일링된 애플리케이션의 UM 동작을 설명하는 콘텐츠를 출력합니다. 이 연습문제에서는 애플리케이션에서 간단하게 몇 가지를 수정한 다음 `nsys profile` UM 데이터 마이그레이션이 어떻게 동작하는지 살펴봅니다.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu)는 `hostFunction`과 `gpuKernel`을 포함하고 있으며, 둘 다 `2<<24` 요소 벡터의 요소를 숫자 `1`로 초기화하는 데 사용할 수 있습니다. 현재는 호스트 함수나 GPU 커널이 사용되고 있지 않습니다.\n",
    "\n",
    "아래의 4개 질문에 대해 여러분이 방금 UM 동작에 대해 학습한 콘텐츠를 고려할 때 먼저 어떤 유형의 페이지 폴트가 발생해야 하는지 가설을 세운 다음 [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu)를 편집하여 코드베이스에 제공된 2개 함수 중 하나 또는 둘 다를 사용함으로써 시나리오를 생성하고 가설을 테스트해 보세요.\n",
    "\n",
    "가설을 테스트하기 위해 아래의 코드 실행 셀을 사용하여 코드를 컴파일 및 프로파일링하세요. `nsys profile --stats=true` 출력에서 얻은 결과뿐만 아니라 가설도 기록해야 합니다. 여러분은 `nsys profile --stats=true`의 출력에서 다음을 찾아야 합니다.\n",
    "\n",
    "- 출력에 _CUDA 메모리 작동 통계_ 섹션이 있나요?\n",
    "- 있다면 호스트에서 디바이스(HtoD) 마이그레이션과 디바이스에서 호스트(DtoH) 마이그레이션 중 무엇을 나타내나요?\n",
    "- 마이그레이션이 있는 경우, 출력 콘텐츠에 따르면 몇 번의 _연산_이 이루어졌나요? 작은 메모리 마이그레이션 작업이 많다면 온디맨드 페이지 폴트가 발생하고 있으며, 요청된 위치에 페이지 폴트가 있을 때마다 작은 메모리 마이그레이션이 일어난다는 신호입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러분이 탐구할 수 있는 시나리오와 도중에 막힐 경우 참고할 수 있는 해답을 드리겠습니다.\n",
    "\n",
    "- CPU에서만 통합 메모리에 액세스할 때 메모리 마이그레이션 및 페이지 폴트에 대한 증거가 있나요? ([해답](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- GPU에서만 통합 메모리에 액세스할 때 메모리 마이그레이션 및 페이지 폴트에 대한 증거가 있나요? ([해답](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- CPU에서 통합 메모리에 먼저 액세스한 다음 GPU에서 액세스하는 경우 메모리 마이그레이션 및 페이지 폴트에 대한 증거가 있나요? ([해답](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- GPU에서 통합 메모리에 먼저 액세스한 다음 CPU에서 액세스하는 경우 메모리 마이그레이션 및 페이지 폴트에 대한 증거가 있나요? ([해답](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-a9ec-c592-c466-383f.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a9ec-c592-c466-383f.qdrep\"\n",
      "Exporting 5432 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a9ec-c592-c466-383f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    65.1        223379389          1  223379389.0  223379389  223379389  cudaMallocManaged    \n",
      "    34.9        119625547          1  119625547.0  119625547  119625547  cudaDeviceSynchronize\n",
      "     0.0            52302          1      52302.0      52302      52302  cudaLaunchKernel     \n",
      "     0.0             4307          1       4307.0       4307       4307  cudaFree             \n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         29231713        4446   6574.8     2207   163840  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum              Operation            \n",
      " ----------  ----------  -------  -------  -------  ---------------------------------\n",
      " 131072.000        4446   29.481    4.000  980.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    50.5        756899331         42  18021412.6    21036  100137395  poll          \n",
      "    41.8        626286020         40  15657150.5    11761  100072486  sem_timedwait \n",
      "     7.5        111973410        577    194061.4     1285   20971020  ioctl         \n",
      "     0.2          2286102         83     27543.4     1091     719417  mmap          \n",
      "     0.0           706052         77      9169.5     2831      24329  open64        \n",
      "     0.0           146410          4     36602.5    28890      41484  pthread_create\n",
      "     0.0           123270         23      5359.6     1180      21688  fopen         \n",
      "     0.0           110368         12      9197.3     3825      15277  write         \n",
      "     0.0            87585          3     29195.0    21883      38802  fgets         \n",
      "     0.0            39836          5      7967.2     3168      12454  open          \n",
      "     0.0            31807         12      2650.6     1061       4595  read          \n",
      "     0.0            31387         16      1961.7     1027       3985  fclose        \n",
      "     0.0            30411          9      3379.0     1270       5117  munmap        \n",
      "     0.0            14821          2      7410.5     6212       8609  socket        \n",
      "     0.0            14272          3      4757.3     4399       5072  pipe2         \n",
      "     0.0            10316          3      3438.7     1120       7278  fgetc         \n",
      "     0.0             7747          4      1936.8     1053       4470  fcntl         \n",
      "     0.0             7646          1      7646.0     7646       7646  connect       \n",
      "     0.0             7228          4      1807.0     1521       2011  mprotect      \n",
      "     0.0             6598          2      3299.0     3017       3581  fread         \n",
      "     0.0             2561          1      2561.0     2561       2561  bind          \n",
      "     0.0             1325          1      1325.0     1325       1325  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report5.qdrep\"\n",
      "Report file moved to \"/dli/task/report5.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 벡터 추가 프로그램을 위한 UM 동작 다시 알아보기\n",
    "\n",
    "이 실습 내내 여러분이 작업 중인 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 프로그램으로 돌아가서 현재 상태의 코드베이스를 검토하고, 발생할 것으로 예상되는 메모리 마이그레이션 및 페이지 폴트의 종류에 대한 가설을 세우세요. 마지막 리팩터의 프로파일링 출력을 살펴보고(위로 스크롤하거나 아래에 있는 코드 실행 셀을 실행하여 출력 확인) 프로파일러 출력의 _CUDA 메모리 작동 통계_ 섹션을 관찰하세요. 코드베이스 콘텐츠를 기반으로 마이그레이션의 종류와 연산 수를 설명할 수 있나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-20ae-6add-9b90-ea83.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-20ae-6add-9b90-ea83.qdrep\"\n",
      "Exporting 12068 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-20ae-6add-9b90-ea83.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    61.9        247851215          3   82617071.7      18274  247769901  cudaMallocManaged    \n",
      "    33.2        132834718          1  132834718.0  132834718  132834718  cudaDeviceSynchronize\n",
      "     4.9         19619586          3    6539862.0    5899339    7674317  cudaFree             \n",
      "     0.0            62312          1      62312.0      62312      62312  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        132821553          1  132821553.0  132821553  132821553  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    79.6         82265458       10219   8050.2     1855   164928  [CUDA Unified Memory memcpy HtoD]\n",
      "    20.4         21104288         768  27479.5     1567   160032  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000       10219   38.479    4.000   988.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    53.0       1325748824         72  18413178.1    21397  100129743  poll          \n",
      "    42.1       1054209454         66  15972870.5    10525  100071143  sem_timedwait \n",
      "     3.9         98681341        601    164195.2     1011   21238354  ioctl         \n",
      "     0.9         22287540         90    247639.3     1095    7591584  mmap          \n",
      "     0.0           725241         77      9418.7     2964      24586  open64        \n",
      "     0.0           201842         11     18349.3     4562      44258  write         \n",
      "     0.0           159799         23      6947.8     1690      26754  fopen         \n",
      "     0.0           155300          4     38825.0    26236      50060  pthread_create\n",
      "     0.0           123807          3     41269.0    31621      57238  fgets         \n",
      "     0.0            53338         14      3809.9     1320       6698  munmap        \n",
      "     0.0            45510          5      9102.0     3652      12537  open          \n",
      "     0.0            38385         16      2399.1     1220       4862  fclose        \n",
      "     0.0            26418         12      2201.5     1052       4812  read          \n",
      "     0.0            18989          2      9494.5     7829      11160  socket        \n",
      "     0.0            15423          3      5141.0     3994       5873  pipe2         \n",
      "     0.0            12079          1     12079.0    12079      12079  connect       \n",
      "     0.0            11735          2      5867.5     2022       9713  fgetc         \n",
      "     0.0             9446          4      2361.5     1955       2970  mprotect      \n",
      "     0.0             7818          2      3909.0     3784       4034  fread         \n",
      "     0.0             7011          4      1752.8     1030       3614  fcntl         \n",
      "     0.0             3002          1      3002.0     3002       3002  bind          \n",
      "     0.0             1919          1      1919.0     1919       1919  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report6.qdrep\"\n",
      "Report file moved to \"/dli/task/report6.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 커널의 벡터 초기화\n",
    "\n",
    "`nsys profile`이 커널을 실행하는 데 소요되는 시간을 알려줄 때, 표시되는 실행 시간에는 이 커널의 실행 중에 발생하는 HtoD 페이지 폴트 및 데이터 마이그레이션이 포함됩니다.\n",
    "\n",
    "이를 염두에 두고 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 프로그램에 있는 `initWith` 호스트 함수를 대신 GPU에서 병렬로 할당된 벡터를 초기화하는 CUDA 커널이 되도록 리팩터링하세요. 리팩터링된 애플리케이션을 성공적으로 컴파일 및 실행한 후, 프로파일링하기 전 다음에 대한 가설을 세우세요.\n",
    "\n",
    "- 리팩터가 UM 메모리 마이그레이션 동작에 어떤 영향을 미칠 것으로 예상되나요?\n",
    "- 리팩터가 보고된 `addVectorsInto`의 런타임에 어떤 영향을 미칠 것으로 예상되나요?\n",
    "\n",
    "다시 한 번 말씀드리지만, 결과를 기록해 두세요. 도중에 막히면 [해답](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu)을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-6cf9-39cd-c792-f419.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6cf9-39cd-c792-f419.qdrep\"\n",
      "Exporting 1784 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6cf9-39cd-c792-f419.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    81.8        258280111          3  86093370.3     26972  258161567  cudaMallocManaged    \n",
      "    10.5         33072807          1  33072807.0  33072807   33072807  cudaDeviceSynchronize\n",
      "     7.6         24062450          3   8020816.7   6274311   11395694  cudaFree             \n",
      "     0.0           145749          4     36437.3      7948      75089  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    95.0         31501042          3  10500347.3  10281717  10816746  initWith(float, float*, int)               \n",
      "     5.0          1647035          1   1647035.0   1647035   1647035  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21137600         768  27522.9     1631   159872  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    46.2        611333662         36  16981490.6    18573  100131042  poll          \n",
      "    44.6        590229820         36  16395272.8    10471  100067297  sem_timedwait \n",
      "     7.0         92079694        591    155803.2     1019   16215483  ioctl         \n",
      "     2.0         26600245         90    295558.3     1100   11294921  mmap          \n",
      "     0.1           901951         77     11713.6     2854      29282  open64        \n",
      "     0.0           179112          4     44778.0    25190      66019  pthread_create\n",
      "     0.0           128503         23      5587.1     1509      22202  fopen         \n",
      "     0.0           120408         11     10946.2     3583      22244  write         \n",
      "     0.0            86397          3     28799.0    20660      38004  fgets         \n",
      "     0.0            55368         14      3954.9     1305       6418  munmap        \n",
      "     0.0            43305          5      8661.0     2789      15502  open          \n",
      "     0.0            33857         16      2116.1     1036       4149  fclose        \n",
      "     0.0            32891         10      3289.1     1009       6435  read          \n",
      "     0.0            16610          2      8305.0     8082       8528  socket        \n",
      "     0.0            16011          3      5337.0     4275       6708  pipe2         \n",
      "     0.0            14927         10      1492.7     1010       4488  fcntl         \n",
      "     0.0             9298          3      3099.3     1914       4724  fread         \n",
      "     0.0             8444          2      4222.0     1832       6612  fgetc         \n",
      "     0.0             8096          1      8096.0     8096       8096  connect       \n",
      "     0.0             7000          4      1750.0     1505       1892  mprotect      \n",
      "     0.0             3109          1      3109.0     3109       3109  bind          \n",
      "     0.0             1595          1      1595.0     1595       1595  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report8.qdrep\"\n",
      "Report file moved to \"/dli/task/report8.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 비동기 메모리 프리페칭\n",
    "\n",
    "**비동기 메모리 프리페칭**은 HtoD 및 DtoH 메모리 전송 둘 다에서 페이지 폴트 및 온디맨드 메모리 마이그레이션의 오버헤드를 줄이는 강력한 기법입니다. 프로그래머는 이 기술을 사용하여 애플리케이션 코드로 백그라운드에서 사용 전 시스템 내 모든 CPU 또는 GPU 디바이스로 통합 메모리(UM)를 비동기 마이그레이션할 수 있습니다. 이렇게 하면 페이지 폴트 및 온디맨드 데이터 마이그레이션 오버헤드가 감소하여 GPU 커널 및 CPU 함수 성능이 향상될 수 있습니다.\n",
    "\n",
    "또한 프리페칭은 데이터를 더 큰 덩어리로 마이그레이션하는 경향이 있으므로 온디맨드 마이그레이션보다 더 적게 이동할 수 있습니다. 따라서 런타임 전에 데이터 액세스 요구 사항을 알 수 있으며 데이터 액세스 패턴이 희소하지 않을 때 탁월한 적합성을 보입니다.\n",
    "\n",
    "CUDA를 이용하면 `cudaMemPrefetchAsync` 함수를 통해 관리되는 메모리를 GPU 디바이스 또는 CPU에 쉽게 비동기 프리페칭할 수 있습니다. 다음은 현재 활성 GPU 디바이스와 CPU에 데이터를 프리페칭하는 데 이를 사용하는 예입니다.\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 메모리 프리페치\n",
    "\n",
    "이 시점의 실습에서 여러분의 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 프로그램은 CUDA 커널을 실행하여 2개 벡터를 `cudaMallocManaged`로 할당된 세 번째 솔루션 벡터에 추가해야 할 뿐만 아니라, 3개 벡터를 병렬로 각각 CUDA 커널에서 초기화해야 합니다. 어떤 이유로든 애플리케이션이 위의 작업을 하지 않는 경우 [참조 애플리케이션](../edit/08-prefetch/01-vector-add-prefetch.cu)을 참고하셔서 여러분의 코드베이스가 현재 기능을 반영하도록 업데이트하세요.\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 애플리케이션 안의 `cudaMemPrefetchAsync`를 사용하여 세 번의 실험을 진행하며 페이지 폴트 및 메모리 마이그레이션에 미치는 영향을 이해해 보세요.\n",
    "\n",
    "- 초기화된 벡터 1개를 디바이스에 프리페치하면 어떤 일이 일어나나요?\n",
    "- 초기화된 벡터 2개를 디바이스에 프리페치하면 어떤 일이 일어나나요?\n",
    "- 초기화된 벡터 3개 모두를 디바이스에 프리페치하면 어떤 일이 일어나나요?\n",
    "\n",
    "각 실험 전 UM 동작, 특히 페이지 폴트와 초기화 커널의 보고된 런타임에 미칠 영향에 대한 가설을 세우고, `nsys profile`을 실행하여 확인하세요. 도중에 막히면 [해답](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu)을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-b050-9b49-a9ac-5e0d.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-b050-9b49-a9ac-5e0d.qdrep\"\n",
      "Exporting 1164 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-b050-9b49-a9ac-5e0d.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  --------  ---------------------\n",
      "    55.5         61444229          1  61444229.0  61444229  61444229  cudaDeviceSynchronize\n",
      "    18.7         20727738          1  20727738.0  20727738  20727738  cuMemAllocManaged    \n",
      "    17.4         19280862          3   6426954.0   5376259   8326602  cudaFree             \n",
      "     8.2          9050246          3   3016748.7     13174   8921222  cudaMemPrefetchAsync \n",
      "     0.1            76908          2     38454.0     19030     57878  cudaMallocManaged    \n",
      "     0.0            50941          1     50941.0     50941     50941  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1701739          1  1701739.0  1701739  1701739  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    75.7         65709440         192  342236.7   339840   344576  [CUDA Unified Memory memcpy HtoD]\n",
      "    24.3         21138720         768   27524.4     1631   160128  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768   170.667     4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  --------  ---------  --------------\n",
      "    56.5        974469916         55  17717634.8  10042616  100129280  poll          \n",
      "    39.0        673373586         52  12949492.0   2054198  100070172  sem_timedwait \n",
      "     2.6         45074444         22   2048838.4      1068   23056729  ioctl         \n",
      "     1.1         18959998         10   1895999.8      1077    8238459  mmap          \n",
      "     0.8         13603269          2   6801634.5     28281   13574988  sem_wait      \n",
      "     0.0            40943          1     40943.0     40943      40943  pthread_create\n",
      "     0.0            21712          6      3618.7      1884       5150  munmap        \n",
      "     0.0             8836          1      8836.0      8836       8836  write         \n",
      "     0.0             3158          1      3158.0      3158       3158  read          \n",
      "     0.0             1933          1      1933.0      1933       1933  mprotect      \n",
      "\n",
      "Report file moved to \"/dli/task/report9.qdrep\"\n",
      "Report file moved to \"/dli/task/report9.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습문제: 메모리를 CPU로 다시 프리페치\n",
    "\n",
    "`addVectorInto` 커널의 정확도를 확인하는 함수를 위해 다시 CPU로 프리페치를 추가하세요. 다시 한 번, `nsys`에서 프로파일링하여 확인하기 전에 UM에 미칠 영향에 대해 가설을 세우세요. 도중에 막히면 [해답](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu)을 참고하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-027a-5907-4ca5-8aa9.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-027a-5907-4ca5-8aa9.qdrep\"\n",
      "Exporting 1328 events: [==================================================100%]64%                    ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-027a-5907-4ca5-8aa9.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    50.5        216222653          3  72074217.7     18684  216137001  cudaMallocManaged    \n",
      "    30.7        131566287          7  18795183.9      9952   34000082  cudaMemPrefetchAsync \n",
      "    14.3         61208128          1  61208128.0  61208128   61208128  cudaDeviceSynchronize\n",
      "     4.4         18956081          3   6318693.7   5364346    8024360  cudaFree             \n",
      "     0.0            46313          1     46313.0     46313      46313  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1700428          1  1700428.0  1700428  1700428  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    76.4         65712768         192  342254.0   339904   344768  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.6         20338848          64  317794.5   310976   319776  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    49.6       1135929682         64  17748901.3    21473  100133518  poll          \n",
      "    37.9        867007035         60  14450117.3     9768  100069341  sem_timedwait \n",
      "    10.9        249577344        594    420163.9     1134   33947695  ioctl         \n",
      "     0.9         20962567         91    230357.9     1080    7944612  mmap          \n",
      "     0.6         14108878          2   7054439.0   107635   14001243  sem_wait      \n",
      "     0.0           682978         77      8869.8     2628      33374  open64        \n",
      "     0.0           181710          5     36342.0    25009      46759  pthread_create\n",
      "     0.0           134279         23      5838.2     1372      18989  fopen         \n",
      "     0.0           100469         12      8372.4     3974      14167  write         \n",
      "     0.0            87364          3     29121.3    23659      39343  fgets         \n",
      "     0.0            46866         14      3347.6     1270       5316  munmap        \n",
      "     0.0            38905          5      7781.0     3167      12024  open          \n",
      "     0.0            32619         16      2038.7     1048       4078  fclose        \n",
      "     0.0            29357         12      2446.4     1115       3799  read          \n",
      "     0.0            20326          3      6775.3     1741      11763  fgetc         \n",
      "     0.0            16565          2      8282.5     7730       8835  socket        \n",
      "     0.0            15088          3      5029.3     3628       6159  pipe2         \n",
      "     0.0            10026          5      2005.2     1467       2999  mprotect      \n",
      "     0.0             7573          1      7573.0     7573       7573  connect       \n",
      "     0.0             6817          2      3408.5     3225       3592  fread         \n",
      "     0.0             5555          2      2777.5     1116       4439  fcntl         \n",
      "     0.0             3169          1      3169.0     3169       3169  bind          \n",
      "     0.0             1338          1      1338.0     1338       1338  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report10.qdrep\"\n",
      "Report file moved to \"/dli/task/report10.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비동기 프리페칭을 사용하는 일련의 리팩터 후, 횟수는 적지만 크기는 큰 메모리 전송이 일어났으며 커널 실행 시간이 현저히 감소했음을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 요약\n",
    "\n",
    "이 시점의 실습에서는 다음을 할 수 있습니다.\n",
    "\n",
    "- Nsight Systems 명령줄 도구(**nsys**)를 사용하여 가속 애플리케이션 성능 프로파일링\n",
    "- **스트리밍 멀티프로세서**에 대한 이해를 활용하여 실행 구성 최적화\n",
    "- 페이지 폴트 및 데이터 마이그레이션에 관한 **통합 메모리** 동작 이해\n",
    "- 성능 향상을 위해 **비동기 메모리 프리페치**를 사용하여 페이지 폴트 및 데이터 마이그레이션 감소\n",
    "- 애플리케이션을 빠르게 가속화 및 배포하기 위해 반복 개발 주기 적용\n",
    "\n",
    "학습 콘텐츠를 정리하고 여러분의 애플리케이션 가속화, 최적화, 배포 능력을 강화하기 위해 이 실습의 마지막 연습문제를 진행하세요. 이 과정을 완료한 후 시간과 관심이 있으신 분들은 *고급 콘텐츠* 섹션을 진행하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 최종 연습문제: 가속 SAXPY 애플리케이션 반복 최적화\n",
    "\n",
    "기본적인 가속 [SAXPY](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1) 애플리케이션을 [여기](../edit/09-saxpy/01-saxpy.cu)에서 제공합니다. 현재 컴파일, 실행, `nsys profile`을 이용한 프로파일링 전에 찾아 수정해야 하는 몇 가지 버그가 포함되어 있습니다.\n",
    "\n",
    "버그를 수정하고 애플리케이션을 프로파일링한 후 `saxpy` 커널의 런타임을 기록하고 *반복해서* 애플리케이션 최적화 작업을 하세요. 매 반복이 끝나면 `nsys profile`을 사용하여 코드 변경 콘텐츠가 커널 성능 및 UM 동작에 미친 영향을 확인하세요.\n",
    "\n",
    "이 실습에서 배운 기법을 활용하세요. 학습을 지원하기 위해 강의 초반에 다룬 기술의 특정 정보를 서둘러 찾아보기보다는 가능할 때마다 [기억 탐색](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/)을 활용하세요.\n",
    "\n",
    "여러분의 최종 목표는 `N`을 수정하지 않고 *100us* 이하에서 실행되도록 정확한 `saxpy` 커널을 프로파일링하는 것입니다. 도중에 막힌 경우 [해답](../edit/09-saxpy/solutions/02-saxpy-solution.cu)을 확인하시고, 원하신다면 컴파일 및 프로파일링을 해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \r\n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-800c-1c0a-babd-cc39.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-800c-1c0a-babd-cc39.qdrep\"\n",
      "Exporting 1010 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-800c-1c0a-babd-cc39.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    94.8        220362054          3  73454018.0    28360  220292710  cudaMallocManaged    \n",
      "     3.5          8160846          1   8160846.0  8160846    8160846  cudaDeviceSynchronize\n",
      "     1.0          2398609          3    799536.3   749070     842513  cudaFree             \n",
      "     0.6          1508359          3    502786.3     7364    1364814  cudaMemPrefetchAsync \n",
      "     0.0            48571          1     48571.0    48571      48571  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
      "   100.0           196410          1  196410.0   196410   196410  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    99.7          8246112          24  343588.0   340032   354816  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.3            24574           4    6143.5     1727    10432  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
      " ---------  ----------  --------  --------  --------  ---------------------------------\n",
      " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    45.4        356952079         20  17847604.0     9455  100066154  sem_timedwait \n",
      "    41.7        328004786         24  13666866.1    30938  100119756  poll          \n",
      "    11.7         91989642        589    156179.4     1022   16005324  ioctl         \n",
      "     0.6          4503254         90     50036.2     1071     763041  mmap          \n",
      "     0.5          3684542          3   1228180.7    40402    1875437  sem_wait      \n",
      "     0.1           662174         77      8599.7     2183      24123  open64        \n",
      "     0.0           197529          5     39505.8    26051      61989  pthread_create\n",
      "     0.0           117063         23      5089.7     1155      17092  fopen         \n",
      "     0.0           113421         13      8724.7     4364      15809  write         \n",
      "     0.0            84329          3     28109.7    21631      38019  fgets         \n",
      "     0.0            42104          5      8420.8     2672      13399  open          \n",
      "     0.0            38049         12      3170.8     1197       5320  munmap        \n",
      "     0.0            36570         14      2612.1     1070       5120  read          \n",
      "     0.0            29504         15      1966.9     1058       3701  fclose        \n",
      "     0.0            16351          2      8175.5     6823       9528  socket        \n",
      "     0.0            14528          3      4842.7     4158       5658  pipe2         \n",
      "     0.0             9739          5      1947.8     1469       2840  mprotect      \n",
      "     0.0             8241          1      8241.0     8241       8241  connect       \n",
      "     0.0             7924          2      3962.0     3200       4724  fread         \n",
      "     0.0             7773          2      3886.5     1781       5992  fgetc         \n",
      "     0.0             6256          2      3128.0     1174       5082  fcntl         \n",
      "     0.0             2891          1      2891.0     2891       2891  bind          \n",
      "     0.0             1616          1      1616.0     1616       1616  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report12.qdrep\"\n",
      "Report file moved to \"/dli/task/report12.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
